import { ADR, ADRTemplate, PolicyTrigger, TimelineEvent } from './types';

export const mockADRs: ADR[] = [
  {
    id: 'adr-0001',
    number: 1,
    title: 'Use GPT-4o as primary inference model for production',
    status: 'accepted',
    category: 'model-selection',
    date: '2025-01-15',
    lastUpdated: '2025-01-20',
    authors: ['Sarah Chen', 'Mike Torres'],
    context: 'Our production system requires a foundation model that balances latency, accuracy, and cost for real-time customer-facing applications. We evaluated multiple models across our benchmark suite including response quality, throughput, and cost per 1K tokens. The system handles approximately 50K requests per day with a p99 latency requirement of under 2 seconds.',
    decision: 'We will use GPT-4o as our primary inference model. In the context of production inference for customer-facing applications, facing the need for sub-2s latency with high accuracy, we decided for GPT-4o and against Claude 3.5 Sonnet and Gemini 1.5 Pro, to achieve the best balance of speed, quality, and cost at our scale, accepting the vendor lock-in risk with OpenAI.',
    consequences: 'Positive: 40% latency reduction vs GPT-4, comparable quality scores on our benchmarks, mature API with good reliability. Negative: Vendor dependency on OpenAI, higher cost than open-source alternatives, limited fine-tuning options. We will implement an abstraction layer to enable future model swaps.',
    alternatives: 'Claude 3.5 Sonnet (higher quality but 30% higher latency), Gemini 1.5 Pro (competitive but less mature API), Llama 3.1 70B self-hosted (lowest cost but requires infrastructure investment and 3x latency).',
    relatedADRs: ['adr-0003', 'adr-0005'],
    tags: ['llm', 'inference', 'production', 'openai'],
    complianceFrameworks: ['Model Governance'],
    linkedPRs: ['PR-1234', 'PR-1245'],
    linkedExperiments: ['exp-ml-2025-001'],
    modelVersion: 'gpt-4o-2025-01-13',
    riskClassification: 'high',
    qualityScore: 92,
  },
  {
    id: 'adr-0002',
    number: 2,
    title: 'Adopt ChromaDB as vector store for RAG pipeline',
    status: 'accepted',
    category: 'rag-architecture',
    date: '2025-01-22',
    lastUpdated: '2025-02-01',
    authors: ['Alex Rivera'],
    context: 'Our RAG pipeline requires a vector database to store and retrieve document embeddings efficiently. We need support for metadata filtering, hybrid search, and the ability to handle 10M+ vectors. The solution should integrate well with our Python-based ML stack and support both local development and cloud deployment.',
    decision: 'We will adopt ChromaDB as our vector store for the RAG pipeline. In the context of document retrieval for our knowledge base, facing the need for fast similarity search with metadata filtering, we decided for ChromaDB and against Pinecone, Weaviate, and pgvector, to achieve a good balance of developer experience, performance, and deployment flexibility, accepting the relative immaturity of the project.',
    consequences: 'Positive: Excellent Python SDK, easy local development, supports metadata filtering and hybrid search, open-source with active community. Negative: Younger project with less production track record than Pinecone, may need to migrate if scale requirements exceed capabilities. Neutral: Will need to implement our own backup and monitoring solutions.',
    alternatives: 'Pinecone (fully managed but expensive and proprietary), Weaviate (feature-rich but more complex setup), pgvector (leverages existing Postgres but limited at scale).',
    relatedADRs: ['adr-0001'],
    tags: ['rag', 'vector-db', 'embeddings', 'retrieval'],
    complianceFrameworks: ['GDPR'],
    linkedPRs: ['PR-1267'],
    riskClassification: 'medium',
    qualityScore: 88,
  },
  {
    id: 'adr-0003',
    number: 3,
    title: 'Implement model abstraction layer using LiteLLM',
    status: 'accepted',
    category: 'infrastructure',
    date: '2025-02-01',
    lastUpdated: '2025-02-05',
    authors: ['Sarah Chen', 'Jordan Kim'],
    context: 'Following ADR-0001, we identified the risk of vendor lock-in with our model provider. We need an abstraction layer that allows us to switch between model providers with minimal code changes. This layer should support streaming, function calling, and cost tracking across providers.',
    decision: 'We will implement a model abstraction layer using LiteLLM as the core routing engine. This provides a unified OpenAI-compatible interface across 100+ LLM providers, with built-in support for fallbacks, load balancing, and spend tracking.',
    consequences: 'Positive: Provider-agnostic code, easy A/B testing between models, built-in cost tracking and rate limiting. Negative: Additional dependency and potential point of failure, slight overhead per request (~5ms). We will wrap LiteLLM in our own thin service layer for additional customization.',
    alternatives: 'Custom abstraction (more control but significant engineering investment), LangChain (too heavy for just routing), direct multi-provider integration (no single interface).',
    relatedADRs: ['adr-0001'],
    supersedes: 'adr-0005',
    tags: ['infrastructure', 'abstraction', 'multi-model', 'litellm'],
    riskClassification: 'medium',
    qualityScore: 95,
  },
  {
    id: 'adr-0004',
    number: 4,
    title: 'Use DVC for dataset versioning and ML pipeline tracking',
    status: 'accepted',
    category: 'data-pipeline',
    date: '2025-02-10',
    lastUpdated: '2025-02-10',
    authors: ['Priya Patel'],
    context: 'Our ML team needs to version large datasets (50GB+) alongside code, track data lineage, and reproduce training pipelines. Git alone cannot handle large binary files efficiently, and we need integration with our existing S3 storage.',
    decision: 'We will adopt DVC (Data Version Control) for dataset versioning and pipeline tracking. DVC integrates with Git to version data files using content-addressable storage on S3, while keeping lightweight pointer files in the repository.',
    consequences: 'Positive: Git-like workflow for data, seamless S3 integration, pipeline DAG definitions, experiment tracking. Negative: Learning curve for team members unfamiliar with DVC, requires S3 bucket management, pipeline definitions add repo complexity.',
    alternatives: 'Git LFS (simpler but no pipeline features), MLflow (better for experiments but weaker data versioning), Delta Lake (overkill for our dataset sizes), Pachyderm (too complex for our team size).',
    tags: ['data', 'versioning', 'pipeline', 'dvc', 's3'],
    linkedPRs: ['PR-1301'],
    datasetVersion: 'v2.3.0',
    riskClassification: 'low',
    qualityScore: 90,
  },
  {
    id: 'adr-0005',
    number: 5,
    title: 'Direct OpenAI SDK integration for model calls',
    status: 'superseded',
    category: 'infrastructure',
    date: '2024-12-01',
    lastUpdated: '2025-02-01',
    authors: ['Sarah Chen'],
    context: 'We needed a way to integrate LLM capabilities into our application. At the time, we were only using OpenAI models and needed a quick solution to get to production.',
    decision: 'Use the OpenAI Python SDK directly for all model interactions, with a simple wrapper class for configuration management.',
    consequences: 'This approach got us to production quickly but created tight coupling with OpenAI. As we explored other providers for cost optimization and capability comparison, the direct integration became a bottleneck.',
    supersededBy: 'adr-0003',
    tags: ['infrastructure', 'openai', 'sdk'],
    riskClassification: 'low',
    qualityScore: 75,
  },
  {
    id: 'adr-0006',
    number: 6,
    title: 'Adopt structured prompt templates with Jinja2',
    status: 'accepted',
    category: 'prompt-architecture',
    date: '2025-02-15',
    lastUpdated: '2025-02-15',
    authors: ['Jordan Kim', 'Alex Rivera'],
    context: 'Our prompts are becoming increasingly complex with dynamic content injection, conditional sections, and multi-turn conversation management. Hard-coded f-strings are error-prone and difficult to version, test, and review in PRs.',
    decision: 'We will use Jinja2 templates for all production prompts, stored in a dedicated /prompts directory with version-controlled template files. Each prompt will have an associated test file validating rendering with sample inputs.',
    consequences: 'Positive: Clear separation of prompt logic from application code, reviewable in PRs, testable, supports template inheritance for common patterns. Negative: Additional dependency, team needs to learn Jinja2 syntax, slight runtime overhead for template rendering.',
    tags: ['prompts', 'templates', 'jinja2', 'prompt-engineering'],
    linkedPRs: ['PR-1334'],
    riskClassification: 'low',
    qualityScore: 87,
  },
  {
    id: 'adr-0007',
    number: 7,
    title: 'Implement feature store using Feast',
    status: 'proposed',
    category: 'feature-store',
    date: '2025-02-20',
    lastUpdated: '2025-02-20',
    authors: ['Priya Patel', 'Mike Torres'],
    context: 'Our ML models rely on features computed from multiple data sources. Currently, feature computation is duplicated across training and serving pipelines, leading to training-serving skew. We need a centralized feature store to manage feature definitions, storage, and serving.',
    decision: 'We propose adopting Feast as our feature store, deployed on Kubernetes with a Redis online store and S3/Parquet offline store. Feast will manage feature definitions as code, enabling consistent feature serving across training and inference.',
    consequences: 'Positive: Eliminates training-serving skew, centralized feature definitions, supports both batch and real-time features. Negative: Operational complexity of running Feast on K8s, Redis costs for online store, team onboarding time. Risks: Feast community momentum â€” need to monitor for alternatives.',
    alternatives: 'Tecton (fully managed but expensive), Hopsworks (more features but heavier), custom feature store (full control but significant engineering investment).',
    tags: ['feature-store', 'feast', 'ml-infrastructure', 'kubernetes'],
    riskClassification: 'high',
    qualityScore: 82,
  },
  {
    id: 'adr-0008',
    number: 8,
    title: 'Adopt GDPR-compliant data retention policy for training data',
    status: 'accepted',
    category: 'data-retention',
    date: '2025-01-10',
    lastUpdated: '2025-01-25',
    authors: ['Sarah Chen', 'Legal Team'],
    context: 'As we scale our AI products in the EU market, we must comply with GDPR data minimization and retention requirements. Our training data includes user-generated content that may contain PII. We need clear policies for data retention, anonymization, and deletion that satisfy both ML training needs and regulatory requirements.',
    decision: 'Implement a tiered data retention policy: raw data with PII retained for 90 days max, anonymized data retained for 2 years, aggregated statistics retained indefinitely. All training data pipelines must include a PII detection and anonymization step before long-term storage.',
    consequences: 'Positive: GDPR compliance, reduced data liability, clear data lifecycle. Negative: Potential loss of training signal from anonymization, increased pipeline complexity, storage costs for maintaining both raw and anonymized versions during the 90-day window.',
    tags: ['compliance', 'gdpr', 'data-retention', 'privacy', 'pii'],
    complianceFrameworks: ['GDPR', 'EU AI Act'],
    riskClassification: 'critical',
    qualityScore: 96,
  },
  {
    id: 'adr-0009',
    number: 9,
    title: 'Deploy inference service on AWS with auto-scaling GPU instances',
    status: 'accepted',
    category: 'inference-serving',
    date: '2025-02-05',
    lastUpdated: '2025-02-12',
    authors: ['Mike Torres'],
    context: 'Our inference workload is bursty with 5x traffic spikes during business hours. We need a deployment strategy that can scale GPU resources dynamically while keeping costs manageable during off-peak hours. Current fixed-instance deployment wastes 60% of GPU capacity overnight.',
    decision: 'Deploy inference services on AWS using EKS with Karpenter for GPU node auto-scaling. Use g5.xlarge instances (NVIDIA A10G) as the default, with spot instances for non-critical workloads. Implement request queuing with SQS for traffic smoothing.',
    consequences: 'Positive: 45% cost reduction from auto-scaling, handles traffic spikes gracefully, spot instances for batch inference save additional 60%. Negative: Cold start latency when scaling from zero (mitigated by minimum replica count), complexity of K8s GPU scheduling, spot instance interruption handling.',
    tags: ['inference', 'aws', 'kubernetes', 'gpu', 'auto-scaling'],
    linkedPRs: ['PR-1289', 'PR-1295'],
    riskClassification: 'high',
    qualityScore: 91,
  },
  {
    id: 'adr-0010',
    number: 10,
    title: 'Implement EU AI Act risk classification for all ML models',
    status: 'proposed',
    category: 'compliance',
    date: '2025-02-25',
    lastUpdated: '2025-02-25',
    authors: ['Sarah Chen', 'Legal Team', 'Priya Patel'],
    context: 'The EU AI Act requires classification of AI systems by risk level, with different compliance requirements for each tier. Our portfolio includes models that may fall into high-risk and limited-risk categories. We need a systematic approach to classify and document compliance requirements for each model.',
    decision: 'Implement an internal risk classification framework aligned with EU AI Act categories. Each model deployment must include a risk assessment ADR, technical documentation per Article 11, and human oversight mechanisms for high-risk systems. Integrate risk classification into our CI/CD pipeline as a required check.',
    consequences: 'Positive: Proactive regulatory compliance, systematic risk documentation, audit readiness. Negative: Additional overhead per model deployment, may slow time-to-production for new models, requires ongoing monitoring of regulatory updates.',
    tags: ['compliance', 'eu-ai-act', 'risk-classification', 'governance'],
    complianceFrameworks: ['EU AI Act', 'Model Governance'],
    riskClassification: 'critical',
    qualityScore: 89,
  },
];

export const templates: ADRTemplate[] = [
  {
    id: 'tpl-model-selection',
    name: 'Model Selection Decision',
    description: 'Template for documenting the selection of an AI/ML model for a specific use case.',
    category: 'model-selection',
    fields: {
      context: 'We need to select a model for [use case]. Key requirements include [latency/accuracy/cost constraints]. The model will be used for [production/research/internal] purposes with approximately [volume] requests per [time period].',
      decision: 'In the context of [use case], facing [key constraints], we decided for [chosen model] and against [alternatives], to achieve [desired outcome], accepting [trade-offs].',
      consequences: 'Positive: [benefits]. Negative: [drawbacks]. Risks: [identified risks and mitigations].',
      alternatives: 'We considered: [Model A] ([pros/cons]), [Model B] ([pros/cons]), [Model C] ([pros/cons]).',
    },
    tags: ['model-selection', 'llm', 'ml'],
  },
  {
    id: 'tpl-data-retention',
    name: 'Data Retention Policy',
    description: 'Template for documenting data retention and privacy decisions.',
    category: 'data-retention',
    fields: {
      context: 'Our [system/pipeline] processes [data type] which may contain [sensitive information type]. We must comply with [regulations] and need clear retention policies.',
      decision: 'Implement [retention strategy]: [tier 1 policy], [tier 2 policy]. All data pipelines must include [required processing steps].',
      consequences: 'Positive: [compliance benefits], [risk reduction]. Negative: [potential data loss], [complexity increase].',
      alternatives: '[Alternative 1] ([evaluation]), [Alternative 2] ([evaluation]).',
    },
    tags: ['data-retention', 'privacy', 'compliance'],
  },
  {
    id: 'tpl-rag-architecture',
    name: 'RAG Architecture Decision',
    description: 'Template for documenting RAG pipeline architecture decisions.',
    category: 'rag-architecture',
    fields: {
      context: 'Our [application] requires retrieval-augmented generation for [use case]. Key requirements include [retrieval accuracy/latency/scale]. The knowledge base contains approximately [volume] documents of [type].',
      decision: 'We will implement a RAG pipeline using [vector store] for retrieval, [embedding model] for encoding, and [chunking strategy]. The pipeline will support [hybrid search/metadata filtering/reranking].',
      consequences: 'Positive: [benefits]. Negative: [drawbacks]. We will need to monitor [metrics] and plan for [scaling considerations].',
      alternatives: '[Alternative approach 1], [Alternative approach 2].',
    },
    tags: ['rag', 'retrieval', 'vector-store'],
  },
  {
    id: 'tpl-inference-serving',
    name: 'Inference Serving Strategy',
    description: 'Template for documenting model serving and deployment decisions.',
    category: 'inference-serving',
    fields: {
      context: 'We need to deploy [model/service] for [use case] with [latency/throughput/availability] requirements. Expected traffic pattern is [steady/bursty/scheduled] with [volume] requests.',
      decision: 'Deploy using [infrastructure] with [scaling strategy]. Use [instance type] for [workload type]. Implement [traffic management approach].',
      consequences: 'Positive: [performance/cost benefits]. Negative: [complexity/operational overhead]. Monitoring: [key metrics to track].',
      alternatives: '[Alternative 1], [Alternative 2].',
    },
    tags: ['inference', 'deployment', 'infrastructure'],
  },
  {
    id: 'tpl-prompt-architecture',
    name: 'Prompt Architecture Decision',
    description: 'Template for documenting prompt engineering and management decisions.',
    category: 'prompt-architecture',
    fields: {
      context: 'Our [application/feature] uses prompts for [task]. Current challenges include [prompt management issues]. We need [requirements for prompt system].',
      decision: 'We will adopt [prompt management approach] using [tools/frameworks]. Prompts will be [stored/versioned/tested] using [method].',
      consequences: 'Positive: [benefits]. Negative: [drawbacks]. Team impact: [onboarding/workflow changes].',
      alternatives: '[Alternative 1], [Alternative 2].',
    },
    tags: ['prompts', 'prompt-engineering'],
  },
  {
    id: 'tpl-feature-store',
    name: 'Feature Store Architecture',
    description: 'Template for documenting feature store selection and architecture.',
    category: 'feature-store',
    fields: {
      context: 'Our ML models require features from [data sources]. Current challenges include [training-serving skew/feature duplication/etc]. We need centralized feature management supporting [batch/real-time/both] serving.',
      decision: 'We will adopt [feature store solution] deployed on [infrastructure]. Online store: [technology], offline store: [technology]. Feature definitions will be managed as [code/config].',
      consequences: 'Positive: [benefits]. Negative: [operational complexity/costs]. Migration plan: [approach to migrate existing features].',
      alternatives: '[Alternative 1], [Alternative 2].',
    },
    tags: ['feature-store', 'ml-infrastructure'],
  },
];

export const policyTriggers: PolicyTrigger[] = [
  {
    id: 'pol-001',
    name: 'Database Schema Change',
    description: 'Requires ADR when database schema is modified',
    pattern: 'migrations/*, schema.*, *.sql',
    severity: 'required',
    enabled: true,
  },
  {
    id: 'pol-002',
    name: 'Model Configuration Change',
    description: 'Requires ADR when model configuration or selection changes',
    pattern: 'models/config.*, inference/*.yaml, model_registry/*',
    severity: 'required',
    enabled: true,
  },
  {
    id: 'pol-003',
    name: 'Infrastructure Change',
    description: 'Suggests ADR when infrastructure-as-code files are modified',
    pattern: 'terraform/*, k8s/*, infrastructure/*, *.tf',
    severity: 'warning',
    enabled: true,
  },
  {
    id: 'pol-004',
    name: 'Privacy-Impacting Change',
    description: 'Mandatory review when data handling or PII processing changes',
    pattern: 'data/pipeline/*, privacy/*, anonymization/*',
    severity: 'required',
    enabled: true,
  },
  {
    id: 'pol-005',
    name: 'API Contract Change',
    description: 'Suggests ADR when public API specifications change',
    pattern: 'api/v*/*.yaml, openapi.*, swagger.*',
    severity: 'warning',
    enabled: true,
  },
  {
    id: 'pol-006',
    name: 'Prompt Template Change',
    description: 'Informational when prompt templates are modified',
    pattern: 'prompts/*.j2, prompts/*.jinja2, prompts/*.txt',
    severity: 'info',
    enabled: false,
  },
];

export const timelineEvents: TimelineEvent[] = [
  { id: 'evt-001', adrId: 'adr-0005', adrTitle: 'Direct OpenAI SDK integration for model calls', type: 'created', date: '2024-12-01', description: 'Initial direct OpenAI integration established' },
  { id: 'evt-002', adrId: 'adr-0005', adrTitle: 'Direct OpenAI SDK integration for model calls', type: 'accepted', date: '2024-12-05', description: 'Approved for production use' },
  { id: 'evt-003', adrId: 'adr-0008', adrTitle: 'Adopt GDPR-compliant data retention policy for training data', type: 'created', date: '2025-01-10', description: 'GDPR compliance initiative started' },
  { id: 'evt-004', adrId: 'adr-0001', adrTitle: 'Use GPT-4o as primary inference model for production', type: 'created', date: '2025-01-15', description: 'Model evaluation process initiated' },
  { id: 'evt-005', adrId: 'adr-0001', adrTitle: 'Use GPT-4o as primary inference model for production', type: 'accepted', date: '2025-01-20', description: 'GPT-4o selected after benchmark evaluation' },
  { id: 'evt-006', adrId: 'adr-0002', adrTitle: 'Adopt ChromaDB as vector store for RAG pipeline', type: 'created', date: '2025-01-22', description: 'Vector store evaluation began' },
  { id: 'evt-007', adrId: 'adr-0008', adrTitle: 'Adopt GDPR-compliant data retention policy for training data', type: 'accepted', date: '2025-01-25', description: 'Data retention policy approved by legal' },
  { id: 'evt-008', adrId: 'adr-0003', adrTitle: 'Implement model abstraction layer using LiteLLM', type: 'created', date: '2025-02-01', description: 'Abstraction layer design proposed' },
  { id: 'evt-009', adrId: 'adr-0002', adrTitle: 'Adopt ChromaDB as vector store for RAG pipeline', type: 'accepted', date: '2025-02-01', description: 'ChromaDB adopted after POC validation' },
  { id: 'evt-010', adrId: 'adr-0005', adrTitle: 'Direct OpenAI SDK integration for model calls', type: 'superseded', date: '2025-02-01', description: 'Superseded by ADR-0003 (LiteLLM abstraction)' },
  { id: 'evt-011', adrId: 'adr-0003', adrTitle: 'Implement model abstraction layer using LiteLLM', type: 'accepted', date: '2025-02-05', description: 'LiteLLM integration approved' },
  { id: 'evt-012', adrId: 'adr-0009', adrTitle: 'Deploy inference service on AWS with auto-scaling GPU instances', type: 'created', date: '2025-02-05', description: 'GPU auto-scaling architecture proposed' },
  { id: 'evt-013', adrId: 'adr-0004', adrTitle: 'Use DVC for dataset versioning and ML pipeline tracking', type: 'created', date: '2025-02-10', description: 'DVC adoption proposed' },
  { id: 'evt-014', adrId: 'adr-0004', adrTitle: 'Use DVC for dataset versioning and ML pipeline tracking', type: 'accepted', date: '2025-02-10', description: 'DVC adopted same day after team consensus' },
  { id: 'evt-015', adrId: 'adr-0009', adrTitle: 'Deploy inference service on AWS with auto-scaling GPU instances', type: 'accepted', date: '2025-02-12', description: 'Auto-scaling deployment approved' },
  { id: 'evt-016', adrId: 'adr-0006', adrTitle: 'Adopt structured prompt templates with Jinja2', type: 'created', date: '2025-02-15', description: 'Prompt templating proposal' },
  { id: 'evt-017', adrId: 'adr-0006', adrTitle: 'Adopt structured prompt templates with Jinja2', type: 'accepted', date: '2025-02-15', description: 'Jinja2 templating adopted' },
  { id: 'evt-018', adrId: 'adr-0007', adrTitle: 'Implement feature store using Feast', type: 'created', date: '2025-02-20', description: 'Feature store evaluation started' },
  { id: 'evt-019', adrId: 'adr-0010', adrTitle: 'Implement EU AI Act risk classification for all ML models', type: 'created', date: '2025-02-25', description: 'EU AI Act compliance initiative proposed' },
];

export function getADRById(id: string): ADR | undefined {
  return mockADRs.find(adr => adr.id === id);
}

export function getADRsByStatus(status: string): ADR[] {
  return mockADRs.filter(adr => adr.status === status);
}

export function getADRsByCategory(category: string): ADR[] {
  return mockADRs.filter(adr => adr.category === category);
}

export function searchADRs(query: string): ADR[] {
  const lowerQuery = query.toLowerCase();
  return mockADRs.filter(
    adr =>
      adr.title.toLowerCase().includes(lowerQuery) ||
      adr.context.toLowerCase().includes(lowerQuery) ||
      adr.decision.toLowerCase().includes(lowerQuery) ||
      adr.tags.some(tag => tag.toLowerCase().includes(lowerQuery)) ||
      adr.category.toLowerCase().includes(lowerQuery)
  );
}
